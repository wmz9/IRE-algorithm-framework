{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "from IRE import AdmWIRE\n",
    "from model import Transformer\n",
    "from copy import deepcopy\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "        \n",
    "setup_seed(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1' # 1 or 2 gpus\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "corpus = data.Corpus('wikitext-2')\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz, device):\n",
    "    n = corpus.train.size()[0]\n",
    "    data1 = data[:n]\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data1.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data1 = data1.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data1 = data1.view(bsz, -1).t().contiguous()\n",
    "    return data1.to(device)\n",
    "        \n",
    "def get_batch(source, i, bptt):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i: i+seq_len]\n",
    "    target = source[i+1: i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "\n",
    "def get_lr(it, min_lr, learning_rate, warmup_iters, lr_decay_iters):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_input = len(corpus.dictionary)\n",
    "max_seq_length = 1000\n",
    "epochs = 20000\n",
    "max_iters = 100000\n",
    "d_output = d_input\n",
    "d_model = 128\n",
    "num_heads = 8\n",
    "num_layers = 2\n",
    "d_ff = 512\n",
    "dropout = 0.0\n",
    "batch_size = 32\n",
    "decay_lr = True\n",
    "max_lr = 6e-4\n",
    "min_lr = max_lr / 20\n",
    "warmup_iters = 3000\n",
    "weight_decay = 5e-4\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "###### hyperparametyers for IRE\n",
    "rank = 0.01  # 0.1, 0.01, 0.001\n",
    "prog = 5.0  # 1, 2, 5, ...\n",
    "beta = 0.0  # momentum of Fisher estimate, 0.0.\n",
    "prog_decay = True  # true or false: prog cos decay\n",
    "######\n",
    "\n",
    "log_interval = 10\n",
    "\n",
    "model = Transformer(d_input, d_output, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "base_optimizer = torch.optim.AdamW\n",
    "optimizer = AdmWIRE(model, base_optimizer, rank=rank, prog=prog, beta=beta, prog_decay=prog_decay, \n",
    "                     lr=max_lr, betas=(beta1,beta2), weight_decay=weight_decay, eps=1e-16)\n",
    "\n",
    "train_data = batchify(corpus.train, batch_size, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = []\n",
    "lrs = []\n",
    "\n",
    "iter = 1\n",
    "epoch = 0\n",
    "\n",
    "while iter < max_iters:\n",
    "    \n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    \n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, max_seq_length)):\n",
    "        \n",
    "        data, targets = get_batch(train_data, i, max_seq_length)\n",
    "        \n",
    "        if iter >= warmup_iters and iter % 10 == 1:\n",
    "            # after warm-up phase, estimate the projection each 10 iters\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.enable_grad():\n",
    "                logits = model(data.t()).view(-1, d_input)\n",
    "                samp_dist = torch.distributions.Categorical(logits=logits)\n",
    "                y_sample = samp_dist.sample()\n",
    "                loss = F.cross_entropy(logits, y_sample, ignore_index=-1)\n",
    "                loss.backward()\n",
    "                optimizer.update_mask()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        lr = get_lr(iter, min_lr, max_lr, warmup_iters, max_iters)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        lrs.append(lr)\n",
    "            \n",
    "        output = model(data.t()).view(-1, d_input)\n",
    "        loss = F.cross_entropy(output, targets, ignore_index=-1)\n",
    "        loss.backward()\n",
    "        optimizer.descent_step(lr, max_lr)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        iter += 1 \n",
    "        \n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "\n",
    "    lossf = loss.item()\n",
    "    loss_train.append(lossf) \n",
    "    \n",
    "    if epoch % log_interval == 0:\n",
    "        print(f\"epoch {epoch}: loss {lossf:.4f}, lr {lr:.2e}, time {dt*1000:.2f}ms\")\n",
    "\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "iters = range(1, len(loss_train)+1)\n",
    "plt.plot(iters, loss_train, color ='C0', label='train loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = range(1, len(lrs)+1)\n",
    "plt.plot(iters, lrs, color ='C0', label='train loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}